\documentclass[12pt]{article}

\input{defaultPream}

\usepackage{float}
\usepackage{makecell}

\begin{document}

%----------BEGIN TITLE----------

\title{Constructing a ROCKS Cluster}

\date{\today}

\author{Ryan Wojtyla}

\thispagestyle{empty}

\maketitle

%-----------END TITLE-----------

%----------BEGIN PREPARATION----------

\section{Preparation}

\qq Preparation is key before attempting to build a ROCKS cluster; there are
many steps to take care of beforehand.

\subsection{Backup Data}

\qq One of the most important preliminary steps is to first backup all the data
on the devices that are being updated. I backed up the contents of the CE, minus
{\tt /mnt} and {\tt /cvmfs}, the SE, and both the server and data partitions of
NAS-0 onto NAS-1. For the CE and SE, {\tt /mnt} was excluded because I don't want
to backup both of the NASs' data onto NAS-1. {\tt /cvmfs} was excluded for the CE
because it is externally mounted from CERN, thus eliminating the need for a
local backup. Unlike the CE and SE, NAS-0 does not have NAS-1 mounted, so the
{\tt rsync} command had to remotely copy the NAS-0 files from the CE. Since there
is already an older backup of NAS-0 on NAS-1 and there is not enough space on
NAS-1 to fully copy another backup of NAS-0, all of the newer files on NAS-0
were manually copied into the old backup of NAS-0, effectively updating it.

\begin{itemize}
  \item Backup the CE (command run while logged into the CE):
    \begin{itemize}
    \item {\tt rsync -av --exclude '/mnt*' --exclude '/cvmfs' / /mnt/nas1/CEBackup-20180803}
    \end{itemize}
  \item Backup the SE (command run while logged into the SE):
    \begin{itemize}
    \item {\tt rsync -av --exclude '/mnt' / /mnt/nas1/SEBackup-20180803}
    \end{itemize}
  \item Backup the server partition of NAS-0 (command run while logged into the CE):
    \begin{itemize}
      \item {\tt rsync -av --exclude '/nas0' root@10.1.255.234:/
          /mnt/nas1/nas0-bak-20180730/NAS-0}
      \end{itemize}
\end{itemize}

\qq Ideally, the entirety of one compute node would also be backed up at this
point, but due to catastrophic hardware failures preventing the nodes from being
run (one of the big Tripplite UPSs AND the APC UPS have failed, leaving me with
only one UPS), I'm going to come back to them later.

\subsection{Create Boot Drives}

\qq Once all of the data is secure, the ISO images of the ROCKS rolls can be
downloaded and installed. The necessary ROCKS rolls, available
\href{http://www.rocksclusters.org/downloads/2017-12-01-download-rocks-7-0-manzanita.html
}{here}, are:

\begin{itemize}
  \item kernel
  \item base
  \item core
  \item CentOS
  \item Updates-CentOS
  \item ganglia
  \item htcondor
\end{itemize}

\qq These ISO images may be directly placed on a formated USB using the {\tt dd}
command. The computer-given name of the USB drive of the form
{\tt /dev/sd?} ({\tt /dev/disk?} on MacOS) must be determined. On Linux, the
{\tt lsblk} command may be used to discover this proper name. On Mac, the ``Disk
Utility'' application may be used.

\qq To create the bootable USB, the {\tt dd} command may be used. The USB must be
first unmounted, {\tt sudo umount /dev/sd?}, then the following command may be
run: {\tt sudo dd bs=4M if=INPUT-FILE.iso of=/dev/sd? conv=fdatasync} (for MacOS:
{\tt sudo dd bs=4m if=INPUT-FILE.iso of=/dev/disk? \&\& sync}). The {\tt bs}
option determines the block size of each packet transferred at a time, the input
file is the ISO file of the image to be copied, and the output file is the name of
the USB to be made bootable. Both the {\tt conv=fdatasync} and {\tt \&\& sync}
parts of the commands ensure that all data has been written before the next
packet is sent.

\qq Since coming across eight USBs may be a challenge, the USBs will be created
as they are needed. The first USB to be made is the one containing the
``kernel'' roll.

\qq Actually, the firmware doesn't support booting from USB, so I need to go
get DVDs because CDs are too small.

\qq All the rolls were burnt onto separate DVDs except for the large ``CentOS''
roll, which is more than 7GB, larger than the 4.7GB discs.


\subsection{Ensure NAS-1 Remains Operational}

\qq Because NAS-1 does not need to be updated, and since everyone needs to
access their data on it, it must be kept running and accessible during the
rebuild process. Since NAS-1 has never technically been part of the cluster with
ROCKS, it is independently addressable via its own IP address. This independent
addressability permits NAS-1 to be mounted and ssh-ed into without the rest of
the cluster. It just needs to be left on and connected to the router.

%-----------END PREPARATION-----------

%----------BEGIN INSTALLATION----------

\section{Installation}

\subsection{Kernel Installation}

\qq The instructions say to first use the ``kernel'' disc. The CE was turned on
so that the optical drive has power to open, the disc was quickly inserted, and
the CE was allowed to continue booting. It automatically booted into the disc,
and it applies checks before continuing.

\qq Once it has finished booting, the system language must be selected. After
that, regional information, such as timezone, is entered.

\subsection{Network Configuration}

\qq Select \textit{Network \& Host Name}.

\qq Now, the network must be configured. The option for the 10GB/s ethernet
connection is turned on.

\qq Next, the ``Configure...'' button is selected. Under the ``IPv4 Settings''
tab, the ``Method'' is set to \textit{Manual}, and the IP of the CE,
163.118.42.1, is entered. The netmask, an arbitrary range of IPs for the
network, is set to 25 in order to encompass the IP range
163.118.42.1-163.118.42.127. The gateway is the IP address of the router through
which the computer is connected to the network. It was found by {\tt ssh}-ing
into NAS-1 and running {\tt ip route}, which displays the default gateway IP,
163.118.42.126, and the IP of NAS-1, itself. Provide a DNS server of {\tt
  8.8.8.8} so that the cluster will recognize its hostname and allow the
internet to work. Under the ``IPv6 Settings'' tab, the ``Method'' is set to
\textit{Link-Local Only}.

\qq With this information entered, the \textit{Save} button is pressed. The
ethernet connection may be switched on as before, and the ``Current host name''
in the bottom right of the screen should read ``uscms1.fltech-grid3.fit.edu'',
the cluster's hostname. If it reads something else, enter the correct hostname
on the bottom left of the screen. Once everything here is completed,
\textit{Done} may be selected.

\qq With network configuration out of the way, all the previously grayed-out
options are now full of color! Now the local network needs to be
configured. Select \textit{Cluster Private Network}.

\qq The ``Private Cluster Interface'' dropdown menu is a list of the ethernet
ports, similar to the list for selecting the public network connection. Since
the 10GB/s connection is used for the public connection, the normal ethernet
port is used for the private connection. Refer back to the ``Network \&
Hostname'' section to see which port is occupied, and select that one in
the ``Private Cluster Interface'' dropdown menu. All the other fields should not
need to be altered; the default settings are fine and expected.

\subsection{Rolls}

\qq Now the rolls can be installed! Select ``ROCKS ROLLS''. Here, rolls may be
installed remotely from the ROCKS website. Select \textit{List Available Rolls}
to generate the roll list. Select the following rolls:

\begin{itemize}
  \item base
  \item CentOS
  \item core
  \item ganglia
  \item htcondor
  \item kernel
  \item perl
  \item python
  \item Updates-CentOS-<version>
\end{itemize}

Once all the desired rolls are selected, select \textit{Add Selected
  Rolls}. After the rolls appear in the ``Selected Rolls'' section, select
\textit{Done}.

\qq If no network rolls can be found, change ``Network Roll'' to ``CD/DVD''
Roll, and just install the ``kernel'' roll. The other rolls may be installed later.

\subsection{Cluster Configuration}

\qq Select \textit{CLUSTER CONFIG} to configure the cluster. The fields are
filled as follows:

\begin{center}
  \begin{tabular}{|l|l|}
    \hline
    Cluster Name & USCMS-FIT-Grid \\
    \hline
    Contact & <administrator email> \\
    \hline
    Project URL &
                  \url{https://research.fit.edu/hep/hohlmann-research-group/grid-cluster}
    \\
    \hline
    Latitude/Longitude & N28.0622 W80.6237 \\
    \hline
    Certificate Organization & OSG \\
    \hline
    Certificate Locality & Melbourne \\
    \hline
    Certificate State & Florida \\
    \hline
    Certificate Country & US \\
    \hline
    NTP Servers & pool.ntp.org \\
  \end{tabular}
\end{center}

After the fields have been verified to be correct, select \textit{Done}.

\subsection{Partitioning}

\qq Now the CE must be partitioned. Select \textit{INSTALLATION
  DESTINATION}. Select the larger of the two drives (the smaller one is the hot
spare and it is NOT TO BE TOUCHED), and select \textit{I will configure
  partitioning.} under ``Other Storage Options''. Select \textit{Done} to be
kicked over to Anaconda's partitioning system.

\qq First, the old partition, the CentOS 6 one, was selected, then removed by
selecting the ``-'' at the bottom of the window. Then the \textit{Click here to
  create them automatically} option was selected.

\qq The created {\tt /home} partition was renamed to {\tt /export}, because
NAS-0 is what holds our home directories, and it was given 100GB. The default
{\tt /boot} and {\tt swap} partitions were left untouched, the {\tt /var}
partition was created and given 100GB, and the {\tt /} partition was allocated
the remainder of the available space, 256.76GB.

\qq Once everything is together, select \textit{Done} and carefully review the
changes. If the changes are correct, select \textit{Accept Changes}.

\subsection{Commit}

\qq If everything is undoubtedly confirmed (to the best of your ability), and
you're absolutely ready to destroy everything and build it back up, say a quick
prayer to your deity of choice and select \textit{Begin Installation}. WARNING:
there is no ``confirm'' to \textit{Begin Installation}

\qq Create a root password when prompted, and do NOT setup a user.

\qq After the installation is complete, reboot as per the instructions. After
reboot do not enter any further configuration; simply continue. When presented
with the login prompt, login as root. Congratulations! ROCKS has been installed
onto the CE!

%-----------END INSTALLATION-----------

%----------BEGIN UNIFICATION----------

\section{Unification}

\qq Now that ROCKS is installed onto the CE, it can be installed onto the other
cluster components so that they may be brought into the fold.

\subsection{Nodes}

\qq To begin, login to the CE as root and run {\tt insert-ethers}, the program
that routes network traffic from the nodes to the ROCKS database for processing.
From the menu, select \textit{Compute}. The ``Inserted Applicances'' screen that
appears should initially be blank; nodes must be turned on for them to be
recognized and configured.

\qq Insert the ROCKS Kernel disc into the first node, {\tt compute-1-0}, and
restart it. {\tt compute-1-0} should appear in the ``Inserted Applicances''
menu. The {\tt ()} in the rightmost column will be blank for now. A star, {\tt
  *}, will appear between them when the node requests a kickstart file from the
CE. The node will boot into the Anaconda installation for ROCKS 7.

\subsubsection{Installing ROCKS onto a Compute Node}

\qq The procedure for installing ROCKS on the nodes is simpler than the CE. For
the ``Network \& Host Name'' section, simply enable the only connected ethernet
connection. The ``Current host name'' in the bottom right corner should display
the name of the node, {\tt compute-<x>-<x>}. 

\qq We're trying an entirely new approach. We discovered that if the proper URL
is provided to the ``Installation Source'' in the Anaconda of CERN CentOS 7 (not
the ROCKS Kernel disk), we get a whole bunch of options in the ``Software
Selection'' section; one of which, is ``Compute Node''! We're gonna try to
install CERN CentOS 7 with the ``Compute Node'' software package, and hope that
does something.
 

%-----------END UNIFICATION-----------


%----------BEGIN CONFIGURATION----------

\section{Configuration}

\subsection{Enable SSH}

\qq {\tt ssh} communication with the cluster is paramount in its efficient
administration. Upon initial installation, the {\tt sshd} service ought to
already be running. This can be verified with {\tt service sshd status}. If it's
not running, it can easily set up after a quick internet search.

\subsection{Automatic Mounting of Key Components}



%-----------END CONFIGURATION-----------

%----------BEGIN TROUBLESHOOTING----------

\section{Troubleshooting}

\subsection{Boot Loader Install Failed}

\qq During installation, the boot loader failed to install. Selecting
\textit{no} on the dialogue box will present an ``unknown error'' screen with a
log and a debug option. Selecting ``debug'' kicks the screen over to the
terminal screen.

\qq Navigating to the ``program-log'' tab, the following errors may be found
after the installer attempted to run {\tt grub2-install --no-floppy /dev/sda}:

\begin{itemize}
\item {\tt grub2-install: warning: Attempting to install GRUB to a disk with multiple
    partition labels. This is not supported yet...}
\item {\tt grub2-install: error: embedding is not possible, but this is required
    for cross-disk install.}
\end{itemize}

The decision to try to install to {\tt /dev/sda} is strange because according to
{\tt fdisk -l /dev/sda} and {\tt fdisk -l /dev/sdb}, run in the ``shell'' tab,
reveal that {\tt /dev/sdb1} is marked for boot, whereas no such marking is
present in the single part of {\tt /dev/sda}.

\qq Trying to run {\tt grub2-install --no-floppy /dev/sda} and {\tt grub2-install
  --no-floppy /dev/sdb} produces the same result: {\tt grub2-install: error:
  /usr/lib/grub/i386-pc/modinfo.sh doesn't exist. Please specify --target or
  --directory.} Investigating that path, {\tt /usr/lib/grub} comes up empty. To
find where that file is actually located, since it has to be somewhere if the
installer is running that command, I ran {\tt find / -name modinfo.sh}. It
appears in two places: {\tt /mnt/sysimage/boot/grub2/i386-pc/modinfo.sh} and
{\tt /mnt/sysimage/usr/lib/grub/i386-pc/modinfo.sh}.

\qq I'm trying to run the installer's command again while specifying the
location of the file it needs: {\tt grub2-install --no-floppy
  --directory='/mnt/sysimage/boot/grub2/i386-pc' /dev/sda}. Now it says:
{\tt grub2-install: error: cannot open
  '/mnt/sysimage/boot/grub2/i386-pc/kernel.img': No such file or
  directory.}. Hmm, let's see if that file exists anywhere with: {\tt find /
  -name kernel.img}. Ahh, it's at
{\tt /mnt/sysimage/usr/lib/grub/i386-pc/kernel.img}! Let's run the installer's
command again, but the directory will be changed to
{\tt /mnt/sysimage/usr/lib/grub/i386-pc}:
{\tt grub2-install --no-floppy --directory='/mnt/sysimage/usr/lib/grub/i386-pc'
  /dev/sda}. Ah ha! Now I'm getting the same error as the installer! Let's try
with {\tt /dev/sdb}: {\tt grub2-install --no-floppy
  --directory='/mnt/sysimage/usr/lib/grub/i386-pc /dev/sdb}. Ohh!
{\tt Installation finished. No error reported.}! 

\qq Since the installer crashed, the CE needs to be rebooted. Be sure to swiftly
eject the disc when it first begins to power on so that it tries to boot
normally. It will boot to grub rescue mode because, while grub is installed, it
is not configured correctly.

\qq Running the {\tt ls} command in grub rescue will show that there are two
drives, (hd0) and (hd1), and {\tt set} will reveal that it is trying to boot into
{\tt (hd0)}. Running {\tt ls (hd0)} will show that its file system is unknown,
while {\tt ls (hd1)} shows a recognizable one, such as ext2. The file system of
(hd0) is unknown because it is the hot spare for the RAID; grub is trying to
boot into the hot spare, just as it was trying to do during installation.

\qq The {\tt set} command said that the variable {\tt prefix} was set to
{\tt (hd0)/boot/grub2} and {\tt root} was set to {\tt hd0}. To point those at the
right place I ran the following two commands:

\begin{itemize}
  \item {\tt set prefix=(hd1)/boot/grub2}
  \item {\tt set root=hd1}
\end{itemize}

\qq Instructions online say after fixing the variables to try loading the normal
module with {\tt insmod normal}, but I get the error, {\tt error: file
  '/boot/grub2/i386-pc/normal.mod' not found.}. Running {\tt ls (hd1)/} shows me
nothing! Does that mean there's nothing on the drive I need, or just that the
information isn't quite accessible? Hmm. I guess a restart couldn't hurt more;
let's try that now that the variables are set.

\qq Actually, you know what, I think I discovered the mistake that caused this
whole mess. I decided to check to box to include the hot spare in the
installation process. It has no place here; it's managed strictly by the RAID
card. It got assigned {\tt sda} and messed everything up. I'm just gonna turn all
this off and redo the installation correctly this time.

\subsection{Remote ROCKS Servers Inaccessible}

\qq Normally, the ROCKS rolls are able to be accessed from the remote ROCKS
server in the ``ROCKS ROLLS'' section of the installation. After the kerfuffle
with the boot loader, however, the server seems to be inaccessible even though
the CE has internet, verified by pinging through the console.

\qq I'm going to use the ability of the installer to remotely obtain the rolls
to see if the issue is just with the communicating with the server, or if it's a
larger problem. I've placed all the required rolls on NAS-1, and I'm going to
try accessing them from there.

\qq To do that, the rolls need to be hosted on an HTTP server on NAS-1. I'm
installing and setting up Apache.

\qq When I tried to {\tt yum install httpd}, none of the mirrors worked, so the
installation failed. I tried a {\tt yum update} and a {\tt yum clean all}, but the
clean just confirmed that there was a serious problem. Now yum reports that
{\tt mirrors.centos.org} cannot be resolved. {\tt yum repolist all} also runs into
the same issue; only the 5 recommended steps are listed. 

\qq Turns out {\tt /etc/resolv.conf} was blank, and, amazingly, adding
{\tt nameserver 9.9.9.9} to it fixed the problem. I wonder if that same file is
messed up in the installer.

\qq On the installer, by changing the kernel parameters from ``quiet'' to
``verbose'', or even ``debug'', the booting up of the GUI installer can be seen
and interrupted, providing access to the shell. {\tt ifconfig} shows the four
network ports and the local network, just as it should, but nothing is connected
to the internet. The cables are plugged in, so let's see about enabling the
internet.

\qq The \href{https://wiki.archlinux.org/index.php/Network_configuration}{Arch
  Linux wiki page for network configuration} is very detailed and it seems to be
helpful. I'm going through it.

\qq I've gone through the page, setting the ip manually and editing
{\tt /etc/hosts}, but no luck. I'm going to try the {\tt /etc/resolv.conf} thing
that worked on NAS-1.

\qq The ROCKS installation CD on the CE suddenly has internet!  The IP is
163.118.42.1/25, the broadcast IP is 163.118.42.127, and the gateway IP is
163.118.42.126 ({\tt ip address add 163.118.42.1/25 broadcast + dev
  enp10s0f0}). A route was added: {\tt ip route add default via 163.118.42.126
  dev enp10s0f0}. A nameserver was added to {\tt /etc/resolv.conf},
{\tt nameserver 9.9.9.9}. The installer can be restarted with {\tt anaconda}.

\qq Now the manual installation via anaconda's CLI can begin. 

The locations of several important lines of code:
\begin{tabular}{|c|c|}
  ROCKS Rolls GUI & {\tt /usr/share/anaconda/addons/org\_rocks\_rolls/gui/spokes/RocksRolls.glade} \\
  GUI Function & {\tt /usr/share/anaconda/addons/org\_rocks\_rolls/gui/spokes/RocksRolls.glade} \\
  Actual Function & {\tt /opt/rocks/lib/python2.7/site-packages/rocks/media.py} \\
\end{tabular}

It is difficult to test the individual lines and functions because of so many
references to ``self'' in the code, which indicates an incredible reliance on
being in a special running environment to operate properly.

\qq I'm trying something new. I'm installing ROCKS onto a flash drive on another
machine, then I'm going to {\tt dd} the flash drive image onto the hard drive in
the cluster. Maybe that will work.

\qq Make sure the USB is COMPLETELY blank ({\tt parted /dev/sd<x> mklabel loop}).

\qq To get to the anaconda command line application:
\begin{itemize}
  \item load the Anaconda GUI like normal
  \item Ctl-Alt-F2 to drop to the base terminal of the CD
  \item try to run {\tt anaconda}
  \item when X fails to load properly, select the VNC option
  \item Although Anaconda will continue to not start up properly, the Anaconda
    environment (with the tmux bar at the bottom) will load.
\end{itemize}

\qq Just to try something new, I loaded a ubuntu liveCD onto the CE and had a
poke around. It looks like everything is where it used to be on the partitions
of the original drive, {\tt /dev/sdb2}, specifically the {\tt /root}
partition. A curious note about how those partitions are organized: {\tt
  /dev/sdb2} is formated as a Linux logical volume, and the logical partitions
of that logical volume are found under {\tt /dev/rocks\_uscms1/}. {\tt
  /dev/rocks\_uscms1/root} is the original root file system, and I mounted it
with {\tt mount /dev/rocks\_uscms1/root /mnt/sdb\_root}. Something interesting I
found, though, is the contents of {\tt /mnt/sdb\_root/etc/redhat-release}, which
report that CentOS 7 is installed on the system. Hmm. Did the original install
do something? Additionally, the {\tt /mnt/sdb\_root/boot} directory is
blank. While THAT boot directory is blank, however, the specific boot partition
of {\tt /dev/sdb}, {\tt /dev/sdb1}, is fully populated with an {\tt el7} image
and an updated grub. 

\qq To enable ssh on a Ubuntu LiveCD, install {\tt openssh-server} with {\tt
  sudo apt install openssh-server}, and check to make sure the service is running
with {\tt sudo service ssh status}.

\qq BREAK-THROUGH! After fiddling with GRUB from the LiveCD, namely reinstalling
it onto the boot partition, the CE still refused to boot! This is because the
3Ware card was not configured to be booted into on the boot order section of the
BIOS. This page is accessed by slamming the <Delete> key immediately upon
turning on the CE. The 3Ware card had its boot status elevated, and I was
presented with a fully fleshed out GRUB prompt! (In contrast to the extremely
limited rescue GRUB prompt from earlier.) What is available is summarized in
Table \ref{tab:grubPartitions}.

\begin{figure}[H]
  \label{tab:grubPartitions}
  \caption{A list of the partitions available to us at the GRUB prompt.}
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      Partition & Filesystem & Summary of Contents \\
      \hline
      {\tt (proc)} & {\tt procfs} & none \\
      {\tt (hd0)} & n/a & n/a \\
      {\tt (hd0,msdos2)} & n/a & n/a \\
      {\tt (hd0,msdos1)} & {\tt ext*} & boot partition of CE data drive \\
      {\tt (hd1)} & {\tt ext*} & none \\
      {\tt (hd1,msdos1)} & {\tt ext*} & misc. config files (namely Squid) \\
      {\tt (fd0)} & n/a & n/a \\
      \hline
    \end{tabular}
  \end{center}
\end{figure}

\qq Let's try to boot into the boot partition. Following some instructions from
online, the first step is to load the {\tt ext2} module, {\tt insmod
  ext2}. Next, the root of the prompt needs to be set to the boot partition,
{\tt set root='(hd0,msdos1)'}. If the root was set correctly, the output of the
{\tt ls /} command ought to be identical to the output of {\tt ls
  (hd0,msdos1)}. In the new {\tt /}, there should be two files that start with
{\tt vmlinuz}. We're only concerned with the one that does not have the word
``rescue'' in the name. To load the compiled kernel, {\tt linux <non-rescuse
  vmlinuz>}, then to boot it, {\tt boot}.

\qq Unfortunately, the kernel panics when it tries to boot; the rescue
kernel was also tried to no avail. The rescue kernel has the panic {\tt not
syncing: VFS: Unable to mount root fs on unknown-block(0,0)}. A quick search has
revealed that it might be complaining about not having the {\tt initramfs}
loaded beforehand. We have that file, but how do we load it properly?

\qq Running the setup commands in a different order, while throwing in a couple
new ones, produces different results, but nothing successful.

\begin{figure}[H]
  \label{cmd:grubAttempt1}
  \begin{tabular}{l|l}
    {\tt set pager=1} & \makecell{Slow down screen output\\so that it can be read.} \\
    {\tt set root=(hd0,msdos1)} & \makecell{Set {\tt root} to be whatever the boot
                                  partition is.} \\
    {\tt insmod ext2} & Load the module for dealing with the boot partition's
                        filesystem. \\
    {\tt linux /vmlinuz root=/dev/sda1} & Load the desired {\tt vmlinuz} image,
                                          usually not the ``rescue'' one, and
                                          identify the boot partition on disk. The
                                          device loosely corresponds with {\tt
                                          hd0} and its partition with {\tt
                                          msdos1}. \\
    {\tt initrd /initrd-plymouth.img} & Load the corresponding {\tt initrd}
                                        image. \\
    {\tt boot} & Attempt to boot into the selected image. \\
  \end{tabular}
  \caption{An attempt to boot into the OS from GRUB.}
\end{figure}

Unfortunately, the {\tt pager} setting does not page the output from {\tt boot},
so we still have no clue what's going on after the button's hit.

\qq I wonder what happens if we use one of the {\tt initramfs} files with {\tt
  initrd} instead of just the plain {\tt initrd} image. Also, I'm gonna try to
boot the rescue pair. It worked! I've booted into the rescue image! I think that
might mean the images aren't ENTIRELY busted. 

\qq Something interesting I've noticed: the {\tt /sysroot} directory seems to be
the miscellaneous configuration files from {\tt (hd1,msdos1)} from the
GRUB. We've also got a {\tt /dev} directory with two drives, one with one
partition ({\tt sda}) and another with two ({\tt sdb}). {\tt mount} reveals that
{\tt /dev/sda1} is mounted on {\tt sysroot}, and that its filesystem is {\tt
  ext3}. {\tt /dev/sdb} isn't mounted at all. I've mounted {\tt /dev/sdb1} on a
directory I created, {\tt /mnt/sdb1}, and it's the boot partition of the data
drive, {\tt (hd0,msdos1)} from the GRUB. I tried to mount {\tt /dev/sdb2}, but
it complained that its filesystem was of type {\tt LVM2b3\_member}, which means
that it's the root filesystem of the CE. Unlike the Ubuntu LiveCD, however,
there is no {\tt /dev/rocks} directory where those logical partitions would be
stored. That's enough poking around for now, let's try to load the proper
initramfs-kernel pair.

\qq What I thought was rescue mode is actually emergency mode; the regular
images brought me to the same place. What can I do with emergency mode?

\qq The issue with the logical partitions may be a GRUB configuration
issue. What if GRUB just needs to be configured to look into and properly load
the logical partitions? When the physical boot partition is loaded, the
complaint is that the {\tt /etc/os-release} file is not found. That's fine
because that file is found on the root partition. In our case, however, that
partition is logical. How can we configure GRUB to boot into a logical
partition?

\subsection{Nodes not found in {\tt insert-ethers}}

\qq I found the documentation from when Ankit and Christian installed ROCKS 6 on
all the nodes back in 2014. They said, after running {\tt insert-ethers
  --cabinet=1} on the CE and selecting \textit{Nodes}, they inserted the ROCKS 6
disc into a node and booted into it. When the disc was loaded, the node sent out
a DHCP request that was then picked up by insert-ethers. Let's see about trying
that out on compute-1-0. Since we don't have a Jumbo DVD for ROCKS 7.0, I'm
trying with the ROCKS 7.0 kernel disc. 

\qq The kernel disc booted to the language selection screen as
expected. Unfortunately, however, I seem to be stuck here. The screen requires
that a mouse be used to select the \textit{Continue} button, but the mouse
doesn't work (USB or PS2). *sigh* I'm gonna see about restarting the node and
interrupting the GUI installer like I did before so I can attempt a command line
installation. 

\qq To do a command line install, the GUI installer must be interrupted before
it can start up. When greeted with the first splash screen, press TAB to edit
the kernel settings: change ``quiet'' to ``debug'' so that more text will appear
on the screen during boot which will allow for more time to cancel the installer
startup with control-C. With the GUI installer interrupted, navigate to the
shell and run {\tt anaconda -T} to begin the text-based version of
anaconda.

\qq Be sure to choose an appropriate time zone. Change the \textit{Software
  selection} from ``Minimal Install'' to ``Compute Node''. Select
\textit{Network configuration}, then the correct ethernet device (most
likely {\tt enp4s0f0}). Configure the device to automatically connect after
reboot and to apply configuration in the installer. The defaults for the other
settings ought to be sufficient. Ensure that the software selection was
successful, create a root password, then begin the installation.

\qq The installation failed; it's complaining about an attribute not having the
expected data, ``AttributeError: 'RocksRollsData' object has no attribute
'info'''. Checking back on {\tt insert-ethers}, however, something interesting
has revealed itself! {\tt compute-1-0}, the node on which the installation
failed, is now visible. The ``()'' is empty, which indicates that, while the
node is visible, it has not requested a kickstart file. I'm guessing, if it
worked properly, the ``*'' would appear between the ``()'' on its own. Since
that hasn't happened here, I'm assuming it didn't work, which would make sense
since the installation failed. Let's try again to see what happens.

\qq Alright, new problem: {\tt insert-ethers} won't close and {\tt tmux} window
switching won't work. I guess I ought to restart the system (Ctrl-Alt-Delete
will bring up a GUI shutdown/restart menu). 

\qq After a restart, {\tt insert-ethers} crashes on startup. It reports ``Access
denied for user 'root'@'localhost''', which is more than a little
concerning. What happens if I try a hard restart? Let's find out. No
change. Huh, what an interesting problem to have.

\qq Sam did some MySQL nonsense (detailed in the adminlog) to get a new error:
``error - unable to download kickstart.''. It also suggests to verify that {\tt
  httpd} is running; it, in fact, is. She also found a website that recommends a
course of action in response to this error. Unfortunately, turns out the site is
6 years old, and not of much help.

\qq Some disappointing news: I read through Ankit's old documentation from when
he and Christian were building the cluster with ROCKS 6 back in 2014. They were
trying to set up the wiki and the nodes at the same time, and the MySQL didn't
like that so much. Sounds familiar! They had to reinstall the CE, so that's what
we're gonna go do.

\qq I've backed up the adminlog, thrown in the ROCKS 7 kernel CD, and restarted
the machine. I'm going to follow the installation instructions at the beginning
of this document.

\qq We've thrown the same ROCKS 7 on the node that's on the CE. We were hoping
to get a ROCKS software package ``compute node'' by pointing the installation
source at a specific URL, but the only available one is ``minimal install''. 

\qq We're gonna try installing the node from itself.

\qq As a last ditch effort, I'm going to try to follow the directions exactly on
a brand new node. We started {\tt insert-ethers}, selected ``Compute'', and
booted the new node, {\tt compute-1-2} (the real one), into the Kernel Roll
CD. We have arrived at the initial ROCKS 7.0 screen where we choose to Install
ROCKS or test the media. We've selected the normal ``Install ROCKS'' option.
Upon turning on the network under the ``Network Configuration'' section of the
installer, {\tt insert-ethers} reports having received a DHCP request from {\tt
  compute-0-1}, what {\tt compute-1-2} has decided to call itself (this is
confirmed by what the installer reports to be the ``Current host name''.  We
noticed something very suspicious on the {\tt insert-ethers} page; it said
``Opened kickstart access to {\tt 10.1.1.0/255.255.255.0} network''. Hmm, what
if we point the ``Installation Source'' of the installer at that address?  After
trying the URLs {\tt 10.1.1.0/255.255.255.0} and {\tt 10.1.1.0}, we just threw
in {\tt 255.255.255.0} and it seemed to sit longer than the other ones. While it
was doing its thing, I noticed the ``Cluster Private Network'' section had yet
to be configured. I clicked that section and, after verifying that the default
values were in fact the proper ones, clicked ``Done'', which saved the
configuration. After that had been completed, the grayed-out progress text for
the ``Installation Source'' changed from ``Probing storage...'' to ``Setting up
installation source''. It's been sitting at there in that state for quite some
time. A'ight, it's been sitting here for far too long; we're gonna try the same
thing on the real {\tt compute-1-3}. This time, we'll configure the private
network before messing with the ``Installation Source''.

\qq Alright, we've started up the new node, and its calling itself {\tt
  compute-0-2}. {\tt insert-ethers} has picked it up. We've selected the
``Cluster Network'' section, verified that the following settings are
acceptable:

\begin{figure}[H]
  \caption{The default ``Cluster Network'' settings.}
  \begin{tabular}{|l|l|}
    \hline
    Private Cluster Interface & {\tt enp4s0f1;00:30:48:C2:F4:41} \\
    Private Domain Name & local \\
    MTU & 1500 \\
    IPv4 Address & 10.1.1.1 \\
    IPv4 Netmask & 255.255.255.0 \\
    \hline
  \end{tabular}
\end{figure}

Selecting ``Done'' saves these settings and completes the configuration. Now
let's do the ``Installation Source''. We're gonna point the node at the full
address given to us by {\tt insert-ethers} using HTTP: {\tt
  10.1.1.0/255.255.255.0} (the use of HTTP was garnered from miscellaneous
brief mentions in online forums). When ``Done'' is selected, the ``Installation
Source'' attempt to probe the storage, but it ultimately fails. Let's try just
{\tt 10.1.1.0}. That failed too, now just {\tt 255.255.255.0}. I discovered
something interesting: running {\tt rocks list network} on the CE gives the same
network provided by {\tt insert-ethers}. What if the ``Cluster Private Network''
section needs to be configured so that it matches THAT network? Let's try that
on the real {\tt compute-1-4}.

\qq This one's calling itself {\tt compute-0-3}. I've changed the ``IPv4
Address'' in the ``Cluster Private Network'' section from {\tt 10.1.1.1} to {\tt
  10.1.1.0}. No dice; fails when pointed at {\tt 10.1.1.0/255.255.255.0} and
{\tt 10.1.1.0}. Actually, rereading the documentation, it says that kickstart
files are transferred over HTTPS rather than HTTP. Let's try that setting. Nope,
nothing. I'm gonna send the 

\qq We've made a Hypernews post asking for help. In the mean time, we're putting
ROCKS Kernel on a node again to play with for a bit. When we turned it on, it
complained of a kernel error and said to run {\tt abrt-cli list} to view it. The
reason it gave was ``nobody cared (try booting with the {\tt irqpoll}
option''. A quick search revealed that the error means an interrupt was not
handled, usually a symptom of buggy firmware. To boot with the {\tt irqpoll}
option, which will, when an interrupt is encountered, poll all interrupt
handlers in an attempt to resolve the interruption. 

\qq While it doesn't seem like the CE and the ROCKS node can see each other
(they have both assigned themselves a local IP of 10.1.1.1), they can both see a
machine with local IP 10.1.1.254 (MAC: 00:25:90:33:A3:D8). What is this machine?
It's not NAS-0, the SE (which is off), or the network switch. Also, what's {\tt
  compute-0-0}? Well, mystery solved; I ssh-ed into 10.1.1.254 to find that it
is NAS-1. 

\qq We're gonna try wiping all the stuff {\tt insert-ethers} has done and try
again. The command is {\tt insert-ethers --remove <host name>}, and the list of
host names can be found with {\tt rocks list host}. Now that {\tt insert-ethers}
has been cleared, let's see what we can do with it now. It was also recommended
that {\tt httpd}, {\tt dhcpd}, and some other services be synced and restarted:

\begin{itemize}
\item {\tt service dhcpd restart}
\item {\tt service httpd restart}
\item {\tt service foundation-mysql restart}
\item {\tt service autofs restart}
\item {\tt rocks sync config}
\item {\tt rocks sync users}
\end{itemize}

Just to see what would happen, I then ran {\tt insert-ethers} on the CE and
restarted {\tt compute-1-0}, the node with the ROCKS Kernel on it. Nothing
happened with that, so I booted it back into the ROCKS Kernel CD. This time,
when we turned networking on, it called itself {\tt compute-0-1}! That's still
not {\tt compute-1-0}, but it's closer. Also, {\tt insert-ethers} has seen {\tt
  compute-0-1}. In the Anaconda installer we manually changed the host name from
{\tt compute-0-1} to {\tt compute-1-0}, but that change was not reflected in
{\tt insert-ethers}; we're not even sure if that change was real.

\end{document}
