\documentclass[12pt]{article}

\input{defaultPream}

\usepackage{float}
\usepackage{makecell}
\usepackage[margin=0.59in]{geometry}
\usepackage{textcomp}
\usepackage{tcolorbox}
\usepackage{minted}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{titling}

\begin{document}

%----------BEGIN TITLE----------

\title{Installing Rocks 7 onto a High Throughput Computing Cluster}

\date{\today}

\thispagestyle{empty}

\maketitle

\begin{multicols}{2}

\begin{center}
  \includegraphics[width=0.4\textwidth]{cms.jpg}
\end{center}

\begin{center}
  \includegraphics[width=0.7\textwidth]{osgLogo.jpg}
\end{center}

\begin{center}
  \includegraphics[width=0.5\textwidth]{rocks_cluster_logo.png}
\end{center}

\end{multicols}

\thispagestyle{empty}

\newpage

%-----------END TITLE-----------

%----------BEGIN TABLE OF CONTENTS----------

\tableofcontents

\newpage

%-----------END TABLE OF CONTENTS-----------

%----------BEGIN BACKGROUND----------

\section{Background}

\qq Florida Tech's High Energy Physics Lab's High Throughput Computing Cluster
is a sophisticated piece of computing machinery that has the capacity to not
only aide research at Florida Tech, but around the world. In addition to serving
as the HEP group's primary storage center, the cluster is associated with the
Open Science Grid, which allows researchers from across the globe to run compute
jobs on our machine. The cluster is glued together with the cluster-building Linux distribution
Rocks. Rocks has its base in CentOS with additional features that ease the
creation of computing clusters. 

\begin{multicols}{2}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.4\textwidth]{labeledCluster.jpg}
  \end{center}
  \caption{A labeled picture of the cluster.}
  \label{pic:labeledCluster}
\end{figure}

\columnbreak

\begin{table}[H]
  \caption{Descriptions of each labeled component of the cluster.}
  \begin{center}
    \begin{tabular}{|p{3cm}|p{5cm}|}
      \hline
      Compute\\ Element (CE) & The CE is the head node of the cluster. It manages
                             all the other components, and is where
                             administrators spend most of their time. \\
      \hline
      Storage Element (SE) & The SE is responsible for managing data transfers
                             from CERN. Current scientific data is needed by
                             researchers hoping to run jobs on the cluster. \\
      \hline
      NAS-0 & NAS-0 is the smaller of the cluster's two storage units. Its 16
              750GB hard drives store the home directories of all the users. \\
      \hline
      NAS-1 & NAS-1 is the larger of the cluster's two storage units. The HEP
              group researchers use its 50TB capacity to store and backup their
              scientific data. \\
      \hline
      Nodes & The cluster's 20 compute nodes each have 8 processors ready to
              compute incoming jobs. \\
      \hline
      Uninterruptible Power Supplies (UPSs) & The UPSs are large battery banks
                                              that provide emergency power to
                                              the cluster in the event of brief
                                              power flickers. \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\end{multicols}

%-----------END BACKGROUND-----------

%----------BEGIN PREPARATION----------

\section{Preparation}

\qq Before a major overhaul of any computer system, it is imperative that it
first be properly prepared. The key components must be backed up so that they
may be available for reference while setting up the new system, and the
continued operation of tangential components not participating in the update
must be ensured.

\subsection{Backup Data}

\qq One of the most important preliminary steps is to first backup all the data
on the devices that are being updated. Onto NAS-1, we backed up the contents of
the CE, omitting {\tt /mnt} and {\tt /cvmfs}, the OS (250 GB) and data (1 TB)
drives of the SE, and both the server and data partitions of NAS-0. For the CE
and SE, {\tt /mnt} was excluded because we only want to back up data on the CE
and SE rather than the data of other systems that may be mounted. {\tt /cvmfs}
was excluded for the CE because it is externally mounted from CERN, thus
eliminating the need for a local backup. Unlike the CE and SE, NAS-0 does not
have NAS-1 mounted, so NAS-0's files had to be copied remotely via the CE, on
which NAS-1 is mounted, before being stored in NAS-1. Since there is already an
older backup of NAS-0 on NAS-1 and there is not enough space on NAS-1 to fully
copy another backup of NAS-0, all of the newer files on NAS-0 were manually
copied into the old backup of NAS-0, effectively updating it.

\begin{tcolorbox}[title=Backup the CE (executed in CE), colback=white,
  colframe=black, coltitle=green]
  \begin{minted}{bash}
    rsync -av --exclude '/mnt*' --exclude '/cvmfs' / /mnt/nas1/CEBackup-20180803
  \end{minted}
\end{tcolorbox}

\begin{tcolorbox}[title=Backup the SE (executed in SE), colback=white,
  colframe=black, coltitle=green]
  \begin{minted}{bash}
    rsync -av --exclude '/mnt' / /mnt/nas1/SEBackup-20180803
  \end{minted}  
\end{tcolorbox}

\begin{tcolorbox}[title=Backup the server partition of NAS-0 (executed in CE),
  colback=white, colframe=black, coltitle=green]
  \begin{minted}{bash}
    rsync -av --exclude '/nas0' root@10.1.255.234:/mnt/nas1/nas0-bak-20180730/NAS-0
  \end{minted}
\end{tcolorbox}

\qq Ideally, the entirety of one compute node would also be backed up at this
point, but due to catastrophic hardware failures preventing the nodes from being
run (one of the big Tripplite UPSs AND the APC UPS have failed, leaving me with
only one UPS), I'm going to come back to them later.

\subsection{Burn Boot Disc}

\qq Once all of the data is safely stored away, a bootable DVD must be
created. The ISO for the Rocks kernel can be found on their
\href{http://www.rocksclusters.org/downloads/2017-12-01-download-rocks-7-0-manzanita.html}{website}. Although
there are several rolls (software package ISOs) labeled \textbf{required} on the
website, only the ``kernel'' roll is required; the other rolls will be
downloaded and installed from the installer on the kernel roll. Because the
firmware of the CE does not support USB boot, the 1GB ISO must be burned onto a
DVD.

%\qq Once all of the data is secure, the ISO images of the Rocks rolls can be
%downloaded and installed. The necessary ROCKS rolls, available
%\href{http://www.rocksclusters.org/downloads/2017-12-01-download-rocks-7-0-manzanita.html
%}{here}, are:
%
%\begin{itemize}
%  \item kernel
%  \item base
%  \item core
%  \item CentOS
%  \item Updates-CentOS
%  \item ganglia
%  \item htcondor
%\end{itemize}
%
%\qq These ISO images may be directly placed on a formated USB using the {\tt dd}
%command. The computer-given name of the USB drive of the form
%{\tt /dev/sd?} ({\tt /dev/disk?} on MacOS) must be determined. On Linux, the
%{\tt lsblk} command may be used to discover this proper name. On Mac, the ``Disk
%Utility'' application may be used.
%
%\qq To create the bootable USB, the {\tt dd} command may be used. The USB must be
%first unmounted, {\tt sudo umount /dev/sd?}, then the following command may be
%run: {\tt sudo dd bs=4M if=INPUT-FILE.iso of=/dev/sd? conv=fdatasync} (for MacOS:
%{\tt sudo dd bs=4m if=INPUT-FILE.iso of=/dev/disk? \&\& sync}). The {\tt bs}
%option determines the block size of each packet transferred at a time, the input
%file is the ISO file of the image to be copied, and the output file is the name of
%the USB to be made bootable. Both the {\tt conv=fdatasync} and {\tt \&\& sync}
%parts of the commands ensure that all data has been written before the next
%packet is sent.
%
%\qq Since coming across eight USBs may be a challenge, the USBs will be created
%as they are needed. The first USB to be made is the one containing the
%``kernel'' roll.
%
%\qq Actually, the firmware doesn't support booting from USB, so I need to go
%get DVDs because CDs are too small.
%
%\qq All the rolls were burnt onto separate DVDs except for the large ``CentOS''
%roll, which is more than 7GB, larger than the 4.7GB discs.


\subsection{Ensure NAS-1 Remains Operational}

\qq Because NAS-1 does not need to be updated, and since everyone needs to
access their data, it must be kept running and accessible during the rebuild
process. Since NAS-1 has never technically been part of the cluster with Rocks,
it is independently addressable via its own IP address. This independent
addressability permits NAS-1 to be mounted and remotely accessed via ssh
without the rest of the cluster. It just needs to be left on and connected to
the router.

%-----------END PREPARATION-----------

%----------BEGIN INSTALLATION----------

\section{Installation}

\subsection{Loading the Installer on the CE}

\qq The kernel DVD made earlier will be used to boot the CE into the Anaconda
installer. Before the CE can be booted into the DVD, its boot order must
configured to prioritize booting from its disc drive. It's BIOS can be accessed
by spamming the DELETE key on startup. 

\qq Once the CE's boot order has been properly configured, the kernel DVD may be
inserted and the CE booted. The CE will then boot into the Anaconda installer
after performing several checks.

%\subsection{Kernel Installation}
%
%\qq The instructions say to first use the ``kernel'' disc. The CE was turned on
%so that the optical drive has power to open, the disc was quickly inserted, and
%the CE was allowed to continue booting. It automatically booted into the disc,
%and it applies checks before continuing.
%
%\qq Once it has finished booting, the system language must be selected. After
%that, regional information, such as timezone, is entered.

\subsection{Public Network Configuration}

\qq The first step of the installation process is configuring the network so
that the other necessary software packages can be installed. Select
\textit{Network \& Host Name}, and ensure that the 10GB/s ethernet connection
(ethernet device usually beginning with {\tt enp10}) is turned on. Further
configuration takes place behind the ``Configure...'' button:

\begin{itemize}
\item Under the ``IPv4 Settings'' tab:
  \begin{itemize}
  \item ``Method'' is set to \textbf{Manual}.
  \item The IP is set to the CE's IP: \textbf{163.118.42.1}.
  \item The netmask is set to \textbf{25} to encompass the IP range
    163.118.42.1-163.118.42.127.
  \item The gateway IP is \textbf{163.118.42.126}, and it was found running
    {\tt ip route} in NAS-1.
  \item A DNS server, such as \textbf{8.8.8.8}, must be provided so that
    hostnames, including the CE's, can be resolved.
  \end{itemize}
\item Under the ``IPv6 Settings'' tab:
  \begin{itemize}
  \item ``Method'' is set to \textbf{Link-Local Only}.
  \end{itemize}
\end{itemize}

\qq With this configuration in place, select \textit{Save} and switch on the
ethernet connection. If the ``Current host name'' text box contains
``uscms1.fltech-grid3.fit.edu'', the hostname of the cluster, then the CE should
be connected to the internet, and the installation can continue! 

\subsection{Private Network Configuration}

\qq With internet access obtained, all the previously grayed-out options are now
full of color! The next step is to configure the \textit{Cluster Private
  Network}. The only thing that must be done here, is to verify that the correct
ethernet port is selected. The CE has a 10GB/s ethernet connection for internet
use, it has already been configured, and a 1GB/s ethernet connection (ethernet
device usually beginning with {\tt enp6}) for internal communication with the
other cluster components. The 1GB/s connection should be selected by default,
and the other default settings are correct. This section can usually be closed
without having altered any of the settings.

%\qq Select \textit{Network \& Host Name}.
%
%\qq Now, the network must be configured. The option for the 10GB/s ethernet
%connection is turned on.
%
%\qq Next, the ``Configure...'' button is selected. Under the ``IPv4 Settings''
%tab, the ``Method'' is set to \textit{Manual}, and the IP of the CE,
%163.118.42.1, is entered. The netmask, an arbitrary range of IPs for the
%network, is set to 25 in order to encompass the IP range
%163.118.42.1-163.118.42.127. The gateway is the IP address of the router through
%which the computer is connected to the network. It was found by {\tt ssh}-ing
%into NAS-1 and running {\tt ip route}, which displays the default gateway IP,
%163.118.42.126, and the IP of NAS-1, itself. Provide a DNS server of {\tt
%  8.8.8.8} so that the cluster will recognize its hostname and allow the
%internet to work. Under the ``IPv6 Settings'' tab, the ``Method'' is set to
%\textit{Link-Local Only}.
%
%\qq With this information entered, the \textit{Save} button is pressed. The
%ethernet connection may be switched on as before, and the ``Current host name''
%in the bottom right of the screen should read ``uscms1.fltech-grid3.fit.edu'',
%the cluster's hostname. If it reads something else, enter the correct hostname
%on the bottom left of the screen. Once everything here is completed,
%\textit{Done} may be selected.
%
%\qq With network configuration out of the way, all the previously grayed-out
%options are now full of color! Now the local network needs to be
%configured. Select \textit{Cluster Private Network}.
%
%\qq The ``Private Cluster Interface'' dropdown menu is a list of the ethernet
%ports, similar to the list for selecting the public network connection. Since
%the 10GB/s connection is used for the public connection, the normal ethernet
%port, usually of the form {\tt enp6s0f0}, is used for the private
%connection. Refer back to the ``Network \& Hostname'' section to see which port
%is occupied, and select that one in the ``Private Cluster Interface'' dropdown
%menu. All the other fields should not need to be altered; the default settings
%are fine and expected.

\subsection{Rolls}

\qq Now the rolls can be installed! Rolls are software packages that are
installed alongside the operating system. Select ``ROCKS ROLLS''. Here, rolls
may be installed remotely from the Rocks website. Select \textit{List Available
  Rolls} to generate the roll list. Select the following rolls:

\begin{multicols}{3}

\begin{itemize}
  \item base
  \item CentOS
  \item core
  \item ganglia
  \item htcondor
  \item kernel
  \item perl
  \item python
  \item Updates-CentOS-\textless version\textgreater
\end{itemize}

\end{multicols}

Once all the desired rolls are selected, select \textit{Add Selected
  Rolls}. After the rolls appear in the ``Selected Rolls'' section, select
\textit{Done}.

\subsection{Cluster Configuration}

\qq Select \textit{CLUSTER CONFIG} to configure the cluster. The fields ought to
resemble those in Table \ref{tab:clusterConfig}.

\begin{table}[H]
  \caption{A recommendation on how to fill in the cluster configuration fields.}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Cluster Name & USCMS-FIT-Grid \\
      \hline
      Contact & \textless administrator email\textgreater \\
      \hline
      Project URL &
                    \url{https://research.fit.edu/hep/hohlmann-research-group/grid-cluster}
      \\
      \hline
      Latitude/Longitude & N28.0622 W80.6237 \\
      \hline
      Certificate Organization & Florida Institute of Technology \\
      \hline
      Certificate Locality & Melbourne \\
      \hline
      Certificate State & Florida \\
      \hline
      Certificate Country & US \\
      \hline
      NTP Servers & pool.ntp.org \\
    \hline
    \end{tabular}
  \end{center}
  \label{tab:clusterConfig}
\end{table}

After the fields have been verified to be correct, select \textit{Done}.

\subsection{Partitioning}

\begin{tcolorbox}[title=WARNING, colback=white, colframe=yellow, coltitle=black]
  When installing a new operating system, be SURE that the destination drive is
  correct. Hot spares and data partitions will show up alongside OS drives in
  the installer.
\end{tcolorbox}

\qq Now the CE must be partitioned. Select \textit{INSTALLATION
  DESTINATION}. Select the larger of the two drives (the smaller one is the hot
spare and it is NOT TO BE TOUCHED), and select \textit{I will configure
  partitioning.} under ``Other Storage Options''. Select \textit{Done} to be
kicked over to Anaconda's partitioning system.

\qq First, the old partition, the CentOS 6 one, was selected, then removed by
selecting the ``-'' at the bottom of the window and checking the box for
deleting all file systems. Then the \textit{Click here to
  create them automatically} option was selected.

\qq Since NAS-0 holds the home directories, the default {\tt /home} partition is
renamed to {\tt /export}, and a new {\tt /var} partition is created. The
partition scheme ought to match what is shown in Table \ref{tab:partition}.

%\qq The created {\tt /home} partition was renamed to {\tt /export}, because
%NAS-0 is what holds our home directories, and it was given 100 GiB. The default
%{\tt /boot} and {\tt swap} partitions were left untouched, the {\tt /var}
%partition was created and given 100 GiB, and the {\tt /} partition was allocated
%the remainder of the available space, 256.76 GiB.

\begin{table}[H]
  \caption{The partitioning configuration for a fresh CE.}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Partition Name & Allocated Memory \\
      \hline
      {\tt /export} & 100 GiB \\
      {\tt /var} & 100 GiB \\
      {\tt /boot} & default \\
      {\tt swap} & default \\
      {\tt /} & remaining \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:partition}
\end{table}

\qq Once everything is together, select \textit{Done} and carefully review the
changes. If the changes are correct, select \textit{Accept Changes}.

\subsection{Commit}

\begin{tcolorbox}[title=WARNING, colback=white, colframe=yellow, coltitle=black]
  There is NO ``Are you sure?'' when \textit{Begin Installation} is pressed!
\end{tcolorbox}

\qq If everything is undoubtedly confirmed (to the best of your ability), and
you're absolutely ready to destroy everything and build it back up, say a quick
prayer to your deity of choice and select \textit{Begin Installation}.

\qq Create a root password when prompted, and do NOT setup a user.

\qq After the installation is complete, reboot as per the instructions. After
reboot do not enter any further configuration; simply continue. When presented
with the login prompt, login as root. Congratulations! Rocks has been installed
onto the CE!

%-----------END INSTALLATION-----------

%----------BEGIN UNIFICATION----------

\section{Unification}

\qq Now that Rocks is installed onto the CE, it can be installed onto the other
cluster components so that they may be brought into the fold.

\subsection{Nodes}

\qq To begin, login to the CE as root and run {\tt insert-ethers}, the program
that routes network traffic from the nodes to the Rocks database for processing.
From the menu, select \textit{Compute}. The ``Inserted Applicances'' screen that
appears should initially be blank; nodes must be turned on for them to be
recognized and configured.

\qq Insert the Rocks kernel disc into the first node, {\tt compute-1-0}, and
restart it. {\tt compute-1-0} should appear in the ``Inserted Applicances''
menu. The {\tt ()} in the rightmost column will be blank for now. A star ({\tt
  *}) will appear when the node requests a kickstart file from the
CE. The node will boot into the Anaconda installation for Rocks 7.

\subsubsection{Installing Rocks onto a Compute Node}

\qq The CE hosts a kickstart file that the nodes use to configure their
installation. In order to fetch this file on startup, the nodes must be
configured for PXE network booting. This is achieved by ensuring PXE is at the
top of the boot order in the BIOS.

\begin{tcolorbox}[title=NOTE, colback=white, colframe=blue]
  If the CE is on while this is being completed, when the nodes restart,
  they will immediately grab the kickstart file from the CE regardless if {\tt
    insert-ethers} is running or not. If this is undesired behavior, shutdown the
  CE before configuring the nodes.
\end{tcolorbox}

%\qq To enter a node's BIOS, spam the DELETE key on start up until ``Entering
%SETUP'' is displayed. To modify the boot order, navigate to the ``Boot'' tab and
%follow the on screen directions to place {\tt PCI BEV: IBA GE Slot 0400 v1236}
%(the ethernet port connected to the router; ethernet device {\tt enp4s0f0}) in
%slot 1. Slot 2 should be the CD drive, {\tt IDE CD}, and slot 3 should be the
%regular hard drive, {\tt PCI SCSI}. The remaining boot options can be
%excluded. Once these changes have been made, Save and Exit.

\qq To enter a node's BIOS, spam the DELETE key on startup until ``Entering
SETUP'' is displayed. To modify the boot order, navigate to the ``Boot'' tab and
follow the on screen directions to adjust the boot order so that it matches what
is shown in Table \ref{tab:bootOrderNodes}.

\begin{table}[H]
  \caption{The boot order of the nodes.}
  \begin{center}
    \begin{tabular}{|l|l|l|}
      \hline
      1 & {\tt PCI BEV: IBA GE Slot 0400 v1236} & ethernet (PXE boot) \\
      2 & {\tt IDE CD} & boot from disc drive \\
      3 & {\tt PCI SCSI} & internal hard drive \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:bootOrderNodes}
\end{table}

%\qq Now that the boot order of all the nodes has been corrected, we're going to
%reinstall Rocks onto the CE. With a fresh CE, we can boot the nodes up with {\tt
%  insert-ethers} running and get them sorted properly. 
%
%\qq All the nodes, except for {\tt compute-1-9} and {\tt compute-2-9}, have been
%brought under control of the CE!

\subsubsection{Incorporate NAS-0}

\qq Now that the nodes are in, let's see about bringing NAS-0 into the
fold. Similar to the nodes, its boot order must also be changed so that
ethernet boot is enabled. After spamming the DELETE key on startup and
navigating to the boot menu of the BIOS, the boot order was arranged as is shown
in Table \ref{tab:bootOrderNAS0}.

\begin{table}[H]
  \caption{The proper boot order of NAS-0.}
  \begin{center}
    \begin{tabular}{|l|l|l|}
      \hline
      1 & {\tt PCI BEV: IBA GE Slot 0600 v1236} & ethernet (PXE boot) \\
      2 & {\tt IDE CD} & boot from disc drive \\
      3 & {\tt PCI SCSI: 3ware Storage Controller} & boot from RAID card \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:bootOrderNAS0}
\end{table}

%I placed {\tt PCI BEV: IBA GE Slot 0600
%  v1236} (the ethernet port in use) on top, followed by {\tt IDE CD} and {\tt
%  PCI SCSI: 3ware Storage Controller} (the RAID card).

\qq After that configuration, {\tt insert-ethers} was run on the CE and
\textit{NAS Appliance} was selected. Now we're ready to save our boot options on
NAS-0 and reboot! Let's see if it works.

\qq We're in! NAS-0 has booted into Rock's Anaconda installer via the kickstart
file. {\tt insert-ethers} also recognizes NAS-0! Let's see about going through
the installer.

%--------------------------TROUBLESHOOTING--------------------------------
\qq The most pressing issue is \textit{Installation Destination}. This section
is a bit tricky because all the disks attached to the RAID card are also
displayed and care must be taken to not install the OS on a RAID disk. There are
13 disks listed in the ``Local Standard Disks'' section ({\tt sd[a-m]}) and one
disk listed in the ``Specialized \& Network Disk'' ({\tt mpatha}). That's a
total of 14 disks out of NAS-0's 16. I'm guessing the two unlisted disks are the
two I dedicated to housing the OS, which is unfortunate because those are the
only two we need. Hmm.

\qq Lol, the OS mirror is degraded; one of the drives failed. That's fine; we're
just booting into NAS-0 normally to replace the drive, then we should be good to
go. We'll also be making note of what all the various groups are named to we can
keep track of them in the installer.

\qq REPLACING NAS-0 OS DRIVE: NAS-0's RAID card is accessible via {\tt tw\_cli}. 
We investigated the health of the OS RAID by running {\tt tw\_cli /c0/u0 show},
which reported that the array was degraded; unit {\tt u0-0} was DEGRADED, while
unit {\tt u0-1}, said to be in port 1, was OK. To formally remove the degraded
drive, we ran {\tt tw\_cli maint remove c0 p0}, which said that the port was
empty. This makes sense, since unit {\tt u0-0} does not have a port listed. We
are going to simply remove the drive 0:0 and throw a new drive in its place.
%----------------------------------------------------------------------

\subsubsection{Incorporate SE}

\qq The SE acts as a node (before, it was named {\tt compute-0-0}). So we'll see
about adding it in as a node. First, like the nodes, the SE's boot order must be
modified. The procedure is the same as the nodes, shown in Table
\ref{tab:bootOrderNodes}. However, on the SE, there are two ethernet ports
listed, and we chose the one titled {\tt PCI BEV: IBA GE Slot 0600 v1324}.

\qq Now that the boot order has been fixed, we can run {\tt insert-ethers} on the
CE and select \textit{Compute}. Go back to the SE, and select ``Save and Exit''
to restart with the new boot order. The SE will request the kickstart
file and load the Anaconda installer.

\begin{tcolorbox}[title=ASIDE, colback=white, colframe=blue]
  The installer needs to be told where to install Rocks 7. There are two disks
  on display, a 250GB {\tt sda} and a 1TB {\tt sdb}. I'm gonna boot into the
  current disk to investigate how these drives are put together so I can make an
  informed decision regarding how to install ROCKS 7 (I'm learning from my
  previous mistakes born of haste). I changed the boot order to prioritize the
  hard drive to get where I need to go. The little 250GB drive is, in fact, the OS
  drive, and the large 1TB drive is a dedicated backup drive, so we will be
  installing ROCKS 7 on just the 250GB drive. We changed the boot order back to
  favor PXE and restarted.
\end{tcolorbox}

\qq The installer decided to automatically begin the installation. Some
{\tt /dev/sda} stuff flashed by without any {\tt /dev/sdb} stuff, so it appears
to have figured itself out. 

\begin{tcolorbox}[title=NOTE, colback=white, colframe=blue]
  Do not forget to set the root password during the
  install! If it is not entered before the install is completed, it can be set
  using Rocks commands on the CE: 
  \begin{itemize}
  \item {\tt rocks set host sec\_attr compute-x-x attr=root\_pw} 
  \item {\tt rocks sync host sec\_attr compute-x-x}
  \end{itemize}
\end{tcolorbox}

\subsubsection{Configuring the Nodes}

\begin{tcolorbox}[title=NOTE, colback=white, colframe=blue]
  The root password of the nodes may not have been set (they cannot be
  directly accessed from themselves). They can be accessed, however, from the
  CE. To set the root password, simply ssh into a node from the CE with {\tt ssh
    compute-[0-1]-[0-9]} and run {\tt passwd} to set the root password. 
\end{tcolorbox}

\qq Since the UPSs are having problems providing power for all the nodes at
once, the nodes will be configured in two equally sized groups: {\tt
  compute-*-[0-4]} and {\tt compute-*-[5-9]}.

%-----------END UNIFICATION-----------

%----------BEGIN CONFIGURATION----------

\section{Configuration}

\subsection{Enable SSH}

\qq {\tt ssh} communication with the cluster is paramount in its efficient
administration. Upon initial installation, the {\tt sshd} service ought to
already be running. This can be verified with {\tt service sshd status}. If it's
not running, it can easily set up after a quick internet search.

\subsection{Automatic Mounting of Key Components}

\subsection{HTCondor}

\qq HTCondor is the cluster's job submission and routing software; it receives
jobs and sends them to the nodes to be computed. Since the HTCondor roll was
installed alongside Rocks, HTCondor is available on the CE straight away; {\tt
  condor\_status} shows that HTCondor recognized the nodes' CPUs. Unfortunately,
however, job submission does not immediately work. Submitted jobs are held
without begin computed.

%\qq We've got the bare minimum set up to start playing with condor! Let's see
%what we can do. We installed ROCKS onto the CE with the Condor Roll, so it's
%already on the system. Running a {\tt condor\_status} shows that condor already
%recognizes all the nodes' CPUs! Very nice. 
%
%\qq I'm following the HT-Condor installation instructions provided by OSG
%(https://opensciencegrid.org/docs/compute-element/install-htcondor-ce/). Before
%we begin tackling the installation, the instructions ask us to verify that some
%prerequisites have been performed. The first of which is the creation of two
%user IDs: {\tt condor}, which has already been created, and {\tt gratia}, which
%hasn't yet. It says {\tt gratia} will be created during installation, but, since
%I think HTCondor is already installed from the ROCKS Roll, we might have to
%manually create it. We'll hold off on that for now.
%
%\qq Another prerequisite we need concern ourselves with is the acquisition of a
%new host certificate. We get to request the first hostcert of the new system!
%Exciting! Unfortunately, the hostcerts need to be requested for both the CE and
%SE, so let's see about getting the SE setup first!
%
%\qq We tried to run a condor job on the CE to see what would happen, but it
%didn't work. Turns out, even though the nodes' CPUs are seen by condor, it
%doesn't look like condor is installed on the nodes! They don't have {\tt
%  /etc/condor/config.d/00personal\_condor.config} files. 
%
%\qq After

\subsection{Setup SE}

\qq Several different pieces of software must be installed and configured on the
SE.

\begin{figure}[H]
  \caption{A list of all the software that must be installed on the SE.}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      HDFS & Hadoop Distributed File System for the SE's data drive \\
      xrootd & data storage system\\
      PhEDEx & CERN data routing\\
      squid & caching proxy to aide data transfers\\
      \hline
    \end{tabular}
  \end{center}
\end{figure}

%-----------END CONFIGURATION-----------

\newpage

%----------BEGIN TROUBLESHOOTING----------

\section{Troubleshooting Log}

\subsection{Boot Loader Install Failed}

\qq During the first attempted installation of Rocks 7 onto the CE, the boot
loader failed to install. Selecting \textit{no} on the dialogue box will present
an ``unknown error'' screen with a log and a debug option. Selecting ``debug''
kicks the screen over to the terminal screen.

\qq Navigating to the ``program-log'' tab, the following errors may be found
after the installer attempted to run the following command:

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    grub2-install --no-floppy /dev/sda
  \end{minted}
  \tcblower
    {\tt grub2-install: warning: Attempting to install GRUB to a disk with multiple
    partition labels. This is not supported yet...} \\
    {\tt grub2-install: error: embedding is not possible, but this is required for
    cross-disk install.}
\end{tcolorbox}

The decision to try to install to {\tt /dev/sda} is strange because according to
{\tt fdisk -l /dev/sda} and {\tt fdisk -l /dev/sdb}, both run in the ``shell''
tab, reveal that {\tt /dev/sdb1} is marked for boot, whereas no such marking is
present in the single part of {\tt /dev/sda}.

\qq Attempting to install GRUB onto {\tt /dev/sda} or {\tt /dev/sdb} yields the
same result:

\begin{tcolorbox}[colback=white, colframe=black]
\begin{minted}{bash}
  grub2-install --no-floppy /dev/sda 
\end{minted}
\begin{minted}{bash}
  grub2-install  --no-floppy /dev/sdb 
\end{minted}
\tcblower
{\tt grub2-install: error: /usr/lib/grub/i386-pc/modinfo.sh doesn't
  exist. Please specify --target or --directory.} 
\end{tcolorbox}

Investigating that path, {\tt /usr/lib/grub} comes up empty. To
find where that file is actually located, since it has to be somewhere if the
installer is running that command, the following command was run:

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    find / -name modinfo.sh
  \end{minted}
  \tcblower
  {\tt /mnt/sysimage/boot/grub2/i386-pc/modinfo.sh}\\

  {\tt /mnt/sysimage/usr/lib/grub/i386-pc/modinfo.sh}
\end{tcolorbox}

\qq I'm trying to run the installer's command again while specifying the
location of the file it needs: 

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    grub2-install --no-floppy --directory='/mnt/sysimage/boot/grub2/i386-pc'
    /dev/sda
  \end{minted}
  \tcblower
  {\tt grub2-install: error: cannot open
    '/mnt/sysimage/boot/grub2/i386-pc/kernel.img': No such file or
    directory.}
\end{tcolorbox}

Hmm, let's see if that file exists anywhere with: 

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    find / -name kernel.img
  \end{minted}
  \tcblower
  {\tt /mnt/sysimage/usr/lib/grub/i386-pc/kernel.img}
\end{tcolorbox}

Ah ha! Let's run the installer's command again, but the directory will be
changed to {\tt /mnt/sysimage/usr/lib/grub/i386-pc}: 

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    grub2-install --no-floppy --directory='/mnt/sysimage/usr/lib/grub/i386-pc'
    /dev/sda
  \end{minted}
  \tcblower
  {\tt /mnt/sysimage/usr/lib/grub/i386-pc/kernel.img}
\end{tcolorbox}

Now I'm getting the same error as the installer! Let's try with {\tt
  /dev/sdb}: 

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    grub2-install --no-floppy --directory='/mnt/sysimage/usr/lib/grub/i386-pc
    /dev/sdb'
  \end{minted}
  \tcblower
  {\tt Installation finished. No error reported.}
\end{tcolorbox}

Exciting news!

\qq Since the installer crashed, the CE needs to be rebooted. Be sure to swiftly
eject the disc when it first begins to power on so that it tries to boot
normally. It will boot to grub rescue mode because, while grub is installed, it
is not configured correctly.

\qq Running the {\tt ls} command in grub rescue will show that there are two
drives, (hd0) and (hd1), and {\tt set} will reveal that it is trying to boot into
{\tt (hd0)}. Running {\tt ls (hd0)} will show that its file system is unknown,
while {\tt ls (hd1)} shows a recognizable one, such as ext2. The file system of
(hd0) is unknown because it is the hot spare for the RAID; grub is trying to
boot into the hot spare, just as it was trying to do during installation.

\qq The {\tt set} command said that the variable {\tt prefix} was set to
{\tt (hd0)/boot/grub2} and {\tt root} was set to {\tt hd0}. To point those at the
right place I ran the following two commands:

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    set prefix=(hd1)/boot/grub2
  \end{minted}
  \begin{minted}{bash}
    set root=hd1
  \end{minted}
\end{tcolorbox}

\qq Instructions online say after fixing the variables to try loading the normal
module with {\tt insmod normal}, but I get the error, {\tt error: file
  '/boot/grub2/i386-pc/normal.mod' not found.}. Running {\tt ls (hd1)/} shows me
nothing! Does that mean there's nothing on the drive I need, or just that the
information isn't quite accessible? Hmm. I guess a restart couldn't hurt more;
let's try that now that the variables are set.

\begin{tcolorbox}[title=RESOLUTION, colback=white, colframe=green!40!black]
  Actually, you know what, I think I discovered the mistake that caused this
  whole mess. I decided to check the box to include the hot spare in the
  installation process. It has no place here; it's managed strictly by the RAID
  card. It got assigned {\tt sda} and messed everything up. I'm just gonna turn all
  this off and redo the installation correctly this time.
\end{tcolorbox}

\subsection{Remote Rocks Servers Inaccessible}

\qq Normally, the Rocks rolls are able to be accessed from the remote Rocks
server in the ``ROCKS ROLLS'' section of the installation. After the kerfuffle
with the boot loader, however, the server seems to be inaccessible even though
the CE has internet, verified by pinging through the console.

\qq I'm going to use the ability of the installer to remotely obtain the rolls
to see if the issue is just with the communicating with the server, or if it's a
larger problem. I've placed all the required rolls on NAS-1, and I'm going to
try accessing them from there.

\qq To do that, the rolls need to be hosted on an HTTP server on NAS-1. I'm
installing and setting up Apache.

\qq When I tried to {\tt yum install httpd}, none of the mirrors worked, so the
installation failed. I tried a {\tt yum update} and a {\tt yum clean all}, but the
clean just confirmed that there was a serious problem. Now yum reports that
{\tt mirrors.centos.org} cannot be resolved. {\tt yum repolist all} also runs into
the same issue; only the 5 recommended steps are listed. 

\qq Turns out {\tt /etc/resolv.conf} was blank, and, amazingly, adding
{\tt nameserver 9.9.9.9} to it fixed the problem. I wonder if that same file is
messed up in the installer.

\qq On the installer, by changing the kernel parameters from ``quiet'' to
``verbose'', or even ``debug'', the booting up of the GUI installer can be seen
and interrupted, providing access to the shell. {\tt ifconfig} shows the four
network ports and the local network, just as it should, but nothing is connected
to the internet. The cables are plugged in, so let's see about enabling the
internet.

\qq The \href{https://wiki.archlinux.org/index.php/Network_configuration}{Arch
  Linux wiki page for network configuration} is very detailed and it seems to be
helpful. I'm going through it.

\qq I've gone through the page, setting the ip manually and editing
{\tt /etc/hosts}, but no luck. I'm going to try the {\tt /etc/resolv.conf} thing
that worked on NAS-1.

\qq The Rocks installation CD on the CE suddenly has internet!  The IP is
163.118.42.1/25, the broadcast IP is 163.118.42.127, and the gateway IP is
163.118.42.126 

\begin{tcolorbox}[title=Enable Internet in Boot CD, colback=white,
  colframe=black, coltitle=green]
  \begin{minted}{bash}
    ip address add 163.118.42.1/25 broadcast + dev enp10s0f0
  \end{minted}
  \begin{minted}{bash}
    ip route add default via 163.118.42.126 dev enp10s0f0
  \end{minted}
  \begin{minted}{bash}
    echo "nameserver 9.9.9.9" >> /etc/resolv.conf
  \end{minted}
\end{tcolorbox}

\qq Now the manual installation via anaconda's CLI can begin.  It is difficult
to test individual lines of code and functions because of so many references to
``self'' in the code, which indicates an incredible reliance on being in a
special running environment to operate properly.

\qq I'm trying something new. I'm installing Rocks onto a flash drive on another
machine, then I'm going to {\tt dd} the flash drive image onto the hard drive in
the cluster. Maybe that will work.

\begin{tcolorbox}[title=Completely Wipe USB, colback=white, colframe=black,
  coltitle=green]
  \begin{minted}{bash}
    parted /dev/sd<x> mklabel loop
  \end{minted}
\end{tcolorbox}

\qq To get to the anaconda command line application:
\begin{itemize}
  \item load the Anaconda GUI like normal
  \item Ctl-Alt-F2 to drop to the base terminal of the CD
  \item try to run {\tt anaconda}
  \item when X fails to load properly, select the VNC option
  \item Although Anaconda will continue to not start up properly, the Anaconda
    environment (with the tmux bar at the bottom) will load.
\end{itemize}

\qq Just to try something new, I loaded a ubuntu liveCD onto the CE and had a
poke around. It looks like everything is where it used to be on the partitions
of the original drive, {\tt /dev/sdb2}, specifically the {\tt /root}
partition. A curious note about how those partitions are organized: {\tt
  /dev/sdb2} is formated as a Linux logical volume, and the logical partitions
of that logical volume are found under {\tt /dev/rocks\_uscms1/}. {\tt
  /dev/rocks\_uscms1/root} is the original root file system, and I mounted it
with {\tt mount /dev/rocks\_uscms1/root /mnt/sdb\_root}. Something interesting I
found, though, is the contents of {\tt /mnt/sdb\_root/etc/redhat-release}, which
report that CentOS 7 is installed on the system. Hmm. Did the original install
do something? Additionally, the {\tt /mnt/sdb\_root/boot} directory is
blank. While THAT boot directory is blank, however, the specific boot partition
of {\tt /dev/sdb}, {\tt /dev/sdb1}, is fully populated with an {\tt el7} image
and an updated grub. 

\qq To enable ssh on a Ubuntu LiveCD, install {\tt openssh-server} with {\tt
  sudo apt install openssh-server}, and check to make sure the service is running
with {\tt sudo service ssh status}.

\qq BREAK-THROUGH! After fiddling with GRUB from the LiveCD, namely reinstalling
it onto the boot partition, the CE still refused to boot! This is because the
3Ware card was not configured to be booted into on the boot order section of the
BIOS. This page is accessed by slamming the DELETE key immediately upon
turning on the CE. The 3Ware card had its boot status elevated, and I was
presented with a fully fleshed out GRUB prompt! (In contrast to the extremely
limited rescue GRUB prompt from earlier.) What is available is summarized in
Table \ref{tab:grubPartitions}.

\begin{table}[H]
  \label{tab:grubPartitions}
  \caption{A list of the partitions available to us at the GRUB prompt.}
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      Partition & Filesystem & Summary of Contents \\
      \hline
      {\tt (proc)} & {\tt procfs} & none \\
      {\tt (hd0)} & n/a & n/a \\
      {\tt (hd0,msdos2)} & n/a & n/a \\
      {\tt (hd0,msdos1)} & {\tt ext*} & boot partition of CE data drive \\
      {\tt (hd1)} & {\tt ext*} & none \\
      {\tt (hd1,msdos1)} & {\tt ext*} & misc. config files (namely Squid) \\
      {\tt (fd0)} & n/a & n/a \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\qq Let's try to boot into the boot partition. Following some instructions from
online, the first step is to load the {\tt ext2} module, {\tt insmod
  ext2}. Next, the root of the prompt needs to be set to the boot partition,
{\tt set root='(hd0,msdos1)'}. If the root was set correctly, the output of the
{\tt ls /} command ought to be identical to the output of {\tt ls
  (hd0,msdos1)}. In the new {\tt /}, there should be two files that start with
{\tt vmlinuz}. We're only concerned with the one that does not have the word
``rescue'' in the name. To load the compiled kernel, {\tt linux <non-rescuse
  vmlinuz>}, then to boot it, {\tt boot}.

\qq Unfortunately, the kernel panics when it tries to boot; the rescue
kernel was also tried to no avail. The rescue kernel has the panic {\tt not
syncing: VFS: Unable to mount root fs on unknown-block(0,0)}. A quick search has
revealed that it might be complaining about not having the {\tt initramfs}
loaded beforehand. We have that file, but how do we load it properly?

\qq I wonder what happens if we use one of the {\tt initramfs} files with {\tt
  initrd} instead of just the plain {\tt initrd} image. Also, I'm gonna try to
boot the rescue pair. It worked! I've booted into the rescue image! I think that
might mean the images aren't ENTIRELY busted. 

\qq Something interesting I've noticed: the {\tt /sysroot} directory seems to be
the miscellaneous configuration files from {\tt (hd1,msdos1)} from the
GRUB. We've also got a {\tt /dev} directory with two drives, one with one
partition ({\tt sda}) and another with two ({\tt sdb}). {\tt mount} reveals that
{\tt /dev/sda1} is mounted on {\tt sysroot}, and that its filesystem is {\tt
  ext3}. {\tt /dev/sdb} isn't mounted at all. I've mounted {\tt /dev/sdb1} on a
directory I created, {\tt /mnt/sdb1}, and it's the boot partition of the data
drive, {\tt (hd0,msdos1)} from the GRUB. I tried to mount {\tt /dev/sdb2}, but
it complained that its filesystem was of type {\tt LVM2b3\_member}, which means
that it's the root filesystem of the CE. Unlike the Ubuntu LiveCD, however,
there is no {\tt /dev/rocks} directory where those logical partitions would be
stored. That's enough poking around for now, let's try to load the proper
initramfs-kernel pair.

\qq What I thought was rescue mode is actually emergency mode; the regular
images brought me to the same place. What can I do with emergency mode?

\qq The issue with the logical partitions may be a GRUB configuration
issue. What if GRUB just needs to be configured to look into and properly load
the logical partitions? When the physical boot partition is loaded, the
complaint is that the {\tt /etc/os-release} file is not found. That's fine
because that file is found on the root partition. In our case, however, that
partition is logical. How can we configure GRUB to boot into a logical
partition?

\begin{tcolorbox}[title=RESOLUTION, colback=white, colframe=green!40!black]
  The CE was not getting internet because a DNS server was not specified in the
  ``Network Configuration'' section in the Anaconda installer. Throwing {\tt
    8.8.8.8} in the DNS slot fixed the problem! Internet is back!
\end{tcolorbox}

\subsection{Nodes not found in {\tt insert-ethers}}

\qq I found the documentation from when Ankit and Christian installed Rocks 6 on
all the nodes back in 2014. They said, after running {\tt insert-ethers
  --cabinet=1} on the CE and selecting \textit{Nodes}, they inserted the Rocks 6
disc into a node and booted into it. When the disc was loaded, the node sent out
a DHCP request that was then picked up by insert-ethers. Let's see about trying
that out on compute-1-0. Since we don't have a Jumbo DVD for Rocks 7.0, I'm
trying with the Rocks 7.0 kernel disc. 

\qq The kernel disc booted to the language selection screen as
expected. Unfortunately, however, I seem to be stuck here. The screen requires
that a mouse be used to select the \textit{Continue} button, but the mouse
doesn't work (USB or PS2). *sigh* I'm gonna see about restarting the node and
interrupting the GUI installer like I did before so I can attempt a command line
installation. 

\qq To do a command line install, the GUI installer must be interrupted before
it can start up. When greeted with the first splash screen, press TAB to edit
the kernel settings: change ``quiet'' to ``debug'' so that more text will appear
on the screen during boot which will allow for more time to cancel the installer
startup with control-C. With the GUI installer interrupted, navigate to the
shell and run {\tt anaconda -T} to begin the text-based version of
anaconda.

\qq Be sure to choose an appropriate time zone. Change the \textit{Software
  selection} from ``Minimal Install'' to ``Compute Node''. Select
\textit{Network configuration}, then the correct ethernet device (most
likely {\tt enp4s0f0}). Configure the device to automatically connect after
reboot and to apply configuration in the installer. The defaults for the other
settings ought to be sufficient. Ensure that the software selection was
successful, create a root password, then begin the installation.

\qq The installation failed; it's complaining about an attribute not having the
expected data, ``AttributeError: 'RocksRollsData' object has no attribute
'info'''. Checking back on {\tt insert-ethers}, however, something interesting
has revealed itself! {\tt compute-1-0}, the node on which the installation
failed, is now visible. The ``()'' is empty, which indicates that, while the
node is visible, it has not requested a kickstart file. I'm guessing, if it
worked properly, the ``*'' would appear between the ``()'' on its own. Since
that hasn't happened here, I'm assuming it didn't work, which would make sense
since the installation failed. Let's try again to see what happens.

\qq Alright, new problem: {\tt insert-ethers} won't close and {\tt tmux} window
switching won't work. I guess I ought to restart the system (Ctrl-Alt-Delete
will bring up a GUI shutdown/restart menu). 

\qq After a restart, {\tt insert-ethers} crashes on startup. It reports ``Access
denied for user 'root'@'localhost''', which is more than a little
concerning. What happens if I try a hard restart? Let's find out. No
change. Huh, what an interesting problem to have.

\qq Sam did some MySQL nonsense (detailed in the adminlog) to get a new error:
``error - unable to download kickstart.''. It also suggests to verify that {\tt
  httpd} is running; it, in fact, is. She also found a website that recommends a
course of action in response to this error. Unfortunately, turns out the site is
6 years old, and not of much help.

\qq Some disappointing news: I read through Ankit's old documentation from when
he and Christian were building the cluster with Rocks 6 back in 2014. They were
trying to set up the wiki and the nodes at the same time, and the MySQL didn't
like that so much. Sounds familiar! They had to reinstall the CE, so that's what
we're gonna go do.

\qq I've backed up the adminlog, thrown in the Rocks 7 kernel CD, and restarted
the machine. I'm going to follow the installation instructions at the beginning
of this document.

\qq We've thrown the same Rocks 7 on the node that's on the CE. We were hoping
to get a Rocks software package ``compute node'' by pointing the installation
source at a specific URL, but the only available one is ``minimal install''. 

\qq We're gonna try installing the node from itself.

\qq As a last ditch effort, I'm going to try to follow the directions exactly on
a brand new node. We started {\tt insert-ethers}, selected ``Compute'', and
booted the new node, {\tt compute-1-2} (the real one), into the Kernel Roll
CD. We have arrived at the initial Rocks 7.0 screen where we choose to Install
Rocks or test the media. We've selected the normal ``Install Rocks'' option.
Upon turning on the network under the ``Network Configuration'' section of the
installer, {\tt insert-ethers} reports having received a DHCP request from {\tt
  compute-0-1}, what {\tt compute-1-2} has decided to call itself (this is
confirmed by what the installer reports to be the ``Current host name''.  We
noticed something very suspicious on the {\tt insert-ethers} page; it said
``Opened kickstart access to {\tt 10.1.1.0/255.255.255.0} network''. Hmm, what
if we point the ``Installation Source'' of the installer at that address?  After
trying the URLs {\tt 10.1.1.0/255.255.255.0} and {\tt 10.1.1.0}, we just threw
in {\tt 255.255.255.0} and it seemed to sit longer than the other ones. While it
was doing its thing, I noticed the ``Cluster Private Network'' section had yet
to be configured. I clicked that section and, after verifying that the default
values were in fact the proper ones, clicked ``Done'', which saved the
configuration. After that had been completed, the grayed-out progress text for
the ``Installation Source'' changed from ``Probing storage...'' to ``Setting up
installation source''. It's been sitting at there in that state for quite some
time. A'ight, it's been sitting here for far too long; we're gonna try the same
thing on the real {\tt compute-1-3}. This time, we'll configure the private
network before messing with the ``Installation Source''.

\qq Alright, we've started up the new node, and its calling itself {\tt
  compute-0-2}. {\tt insert-ethers} has picked it up. We've selected the
``Cluster Network'' section, verified that the following settings are
acceptable:

\begin{table}[H]
  \caption{The default ``Cluster Network'' settings.}
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      Private Cluster Interface & {\tt enp4s0f1;00:30:48:C2:F4:41} \\
      Private Domain Name & local \\
      MTU & 1500 \\
      IPv4 Address & 10.1.1.1 \\
      IPv4 Netmask & 255.255.255.0 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

Selecting ``Done'' saves these settings and completes the configuration. Now
let's do the ``Installation Source''. We're gonna point the node at the full
address given to us by {\tt insert-ethers} using HTTP: {\tt
  10.1.1.0/255.255.255.0} (the use of HTTP was garnered from miscellaneous
brief mentions in online forums). When ``Done'' is selected, the ``Installation
Source'' attempt to probe the storage, but it ultimately fails. Let's try just
{\tt 10.1.1.0}. That failed too, now just {\tt 255.255.255.0}. I discovered
something interesting: running {\tt rocks list network} on the CE gives the same
network provided by {\tt insert-ethers}. What if the ``Cluster Private Network''
section needs to be configured so that it matches THAT network? Let's try that
on the real {\tt compute-1-4}.

\qq This one's calling itself {\tt compute-0-3}. I've changed the ``IPv4
Address'' in the ``Cluster Private Network'' section from {\tt 10.1.1.1} to {\tt
  10.1.1.0}. No dice; fails when pointed at {\tt 10.1.1.0/255.255.255.0} and
{\tt 10.1.1.0}. Actually, rereading the documentation, it says that kickstart
files are transferred over HTTPS rather than HTTP. Let's try that setting. Nope,
nothing. I'm gonna send the 

\qq We've made a Hypernews post asking for help. In the mean time, we're putting
Rocks Kernel on a node again to play with for a bit. When we turned it on, it
complained of a kernel error and said to run {\tt abrt-cli list} to view it. The
reason it gave was ``nobody cared (try booting with the {\tt irqpoll}
option''. A quick search revealed that the error means an interrupt was not
handled, usually a symptom of buggy firmware. To boot with the {\tt irqpoll}
option, which will, when an interrupt is encountered, poll all interrupt
handlers in an attempt to resolve the interruption. 

\qq While it doesn't seem like the CE and the Rocks node can see each other
(they have both assigned themselves a local IP of 10.1.1.1), they can both see a
machine with local IP 10.1.1.254 (MAC: 00:25:90:33:A3:D8). What is this machine?
It's not NAS-0, the SE (which is off), or the network switch. Also, what's {\tt
  compute-0-0}? Well, mystery solved; I ssh-ed into 10.1.1.254 to find that it
is NAS-1. 

\qq We're gonna try wiping all the stuff {\tt insert-ethers} has done and try
again. The command is {\tt insert-ethers --remove <host name>}, and the list of
host names can be found with {\tt rocks list host}. Now that {\tt insert-ethers}
has been cleared, let's see what we can do with it now. It was also recommended
that {\tt httpd}, {\tt dhcpd}, and some other services be synced and restarted:

\begin{itemize}
\item {\tt service dhcpd restart}
\item {\tt service httpd restart}
\item {\tt service foundation-mysql restart}
\item {\tt service autofs restart}
\item {\tt rocks sync config}
\item {\tt rocks sync users}
\end{itemize}

Just to see what would happen, I then ran {\tt insert-ethers} on the CE and
restarted {\tt compute-1-0}, the node with the Rocks Kernel on it. Nothing
happened with that, so I booted it back into the Rocks Kernel CD. This time,
when we turned networking on, it called itself {\tt compute-0-1}! That's still
not {\tt compute-1-0}, but it's closer. Also, {\tt insert-ethers} has seen {\tt
  compute-0-1}. In the Anaconda installer we manually changed the host name from
{\tt compute-0-1} to {\tt compute-1-0}, but that change was not reflected in
{\tt insert-ethers}; we're not even sure if that change was real.

\begin{tcolorbox}[title=RESOLUTION, colback=white, colframe=green!40!black]
Daniel Campos is investigating the cluster. He's gotten into the BIOS of
{\tt compute-1-0} by spamming DELETE. The nodes DO have support for
PXE boot! He changed the boot order to prioritize the ethernet port. On the head
node, he set up {\tt tcpdump} to monitor network traffic to see if the head node
will pick up anything from the node. It booted into the Anaconda installer after
downloading the kickstart file from the head node, and it said it got all the
configurations from the kickstart file!
\end{tcolorbox}

% placement of port 69 (TFTP) rules in iptables

\subsection{NAS-0 RAID Issues}

\qq After discovering a drive failure in NAS-0, we tried to replace it with a
new drive and were surprised to find that it could not discover the new
drive. Upon further examination, we found that the newly inserted drive decided
to become a part of unit 1 rather than rebuild itself as part of unit 0, the
mirror array. Now the RAID card thinks it has 1 degraded mirror array and 15
JBOD units; that's a total of 17 drives, even though there are only 16 drives
plugged in. NAS-0 can't do math! Hmm. Let's see what ZFS says about all this.

\qq On startup, we got a ZFS error: the zpool {\tt nas0} couldn't be imported
because \\ {\tt /var/lib/dkms/zfs/0.7.5/source/dkms.conf} doesn't exist. To see if
a {\tt dkms.conf} exists anywhere on NAS-0, we ran: 

\begin{tcolorbox}[colback=white, colframe=black]
  \begin{minted}{bash}
    locate dkms.conf
  \end{minted}
  \tcblower
  {\tt /usr/src/spl-0.7.5/dkms.conf} \\
  {\tt /usr/src/spl-0.7.9/dkms.conf} \\
  {\tt /usr/src/zfs-0.7.9/dkms.conf} \\
  {\tt /var/lib/dkms/zfs/0.7.9/build/dkms.conf} \\ 
\end{tcolorbox}

What's suspicious here, is that several of these files seem to be of a newer
version, 0.7.9 rather than the apparently expected 0.7.5.

\qq To investigate {\tt dkms} itself, we ran {\tt dkms status}, which, again,
reported that the {\tt dkms.conf} file didn't exist, but it also warned that the
built and installed modules are different! I suspect a version conflict because
{\tt dkms status} identifies as version 0.7.5, while all but one of the {\tt
  dkms.conf} files available are version 0.7.9. Additionally, the {\tt /var}
{\tt dkms.conf} file is under a {\tt build} directory, which gives further
credence to {\tt dkms status}'s warning. 

\qq We embarked upon a spelunking expedition to investigate what happened to\\
{\tt /var/lib/dkms/zfs/0.7.5/source/dkms.conf}, and found something of
interest!\\
Within {\tt /var/lib/dkms/zfs/0.7.5}, the {\tt source} ``directory'' is actually
a broken symbolic link pointing to {\tt /usr/src/zfs-0.7.5}, where the other
{\tt dkms.conf}s are found! Unfortunately, however, there is only a {\tt
  /usr/src/zfs-0.7.9} directory, while {\tt /usr/src/zfs-0.7.5} is suspiciously
absent.

\begin{tcolorbox}[title=ASIDE, colback=white, colframe=blue]
  Before proceeding, we performed a quick check to make sure NAS-0's
  data was all nicely backed up on NAS-1. Always make sure the data's backed up
  before messing with where it's stored!
\end{tcolorbox}

\qq We have a question to answer to help us diagnose the cause of the problem:
Is ZFS telling DKMS to look in the wrong spot, or is DKMS looking in the wrong
spot all on its own? Let's find where DKMS is told to check for {\tt dkms.conf}
at {\tt /var/lib/dkms/zfs/0.7.5/source/dkms.conf}. 

\qq Just to see what would happen, we changed the broken symlink in {\tt
  /var/lib/dkms/zfs/0.7.5} from {\tt source -> /usr/src/zfs-0.7.5} to {\tt
  source -> /usr/src/zfs-0.7.9}. {\tt dkms status} now successfully adds the ZFS
module, despite continued warnings proclaiming that the built and installed
modules are different. Although the ZFS module has been added and the commands
work, {\tt zpool list} returns no pools! Huh. Does this have to do with our jank
method of making DKMS work, or is there something more sinister going on? 

\qq Now that {\tt dkms.conf} can be found, let's restart NAS-0 to see what the
ZFS boot message says now. The ZFS error saying it couldn't import the pool {\tt
  nas0} is still there, but DKMS now runs some pre-build script that appears to
hang at {\tt checking spl build directory...}. After sitting there for a while,
it reported that the build directory was {\tt
  /var/lib/dkms/spl/0.7.9/2.6.32-431.11.2.el6.x86\_64/x86\_64}, then threw a
configuration error saying to make sure that the {\tt kmod spl devel <kernel>}
package is installed for our distribution; it failed to find {\tt spl\_config.h}
in either\\ {\tt /usr/src/spl-0.7.9/2.6.32-431.11.2.el6.x86\_64} or {\tt
  /usr/src/spl-0.7.9}. It then failed to build the ZFS module and continued the
normal booting process. 

\qq {\tt dkms status} reports that SPL 0.7.5 and ZFS 0.7.9 have been added, but
it warns that there is a difference between the build and installed modules of
SPL. 

\qq While {\tt zpool status} shows that there are no pools available, {\tt zpool
  import} happily shows us that the {\tt nas0} pool is alive and well in the
ONLINE state. The ``action'' says that ``The pool can be imported using its name
or numeric identifier.'', which implies that the pool need just be imported to
work. {\tt zpool import nas0} seems to have done the trick; {\tt zpool status}
shows the status of the data drives. While all the main drives are reported to
be online, the two hot spares, {\tt sdo} and {\tt sdi}, have failed; they're
state is FAULTED. Another peculiar note is that, while most of the drives are
represented by their SCSI identifiers, two are simply identified by their names,
{\tt sdg} and {\tt sdh}. The drives both have the status ONLINE, however, so
this may not be an issue. Interestingly enough, {\tt tw\_cli /c0 show} does not
show that the two hot spares are damaged; they appear fine to the RAID
card. Perhaps the two drives are merely suffering from a ZFS issue that can be
easily corrected? Additionally, {\tt tw\_cli /c0 show} reports a further drive
failure in {\tt p5}. Unfortunately, it insists on reporting the RAID-1 OS array
as degraded while assigning each individual drive its own individual unit. 

\qq While investigating {\tt /dev/sd*}, we noticed that {\tt /dev/sdb} and the
other drives all had two partitions, {\tt /dev/sdx1} and {\tt /dev/sdx9}, while
{\tt /dev/sda}, the drive we presumably just replaced, only has one partition,
{\tt /dev/sda1}. 

\qq The root issue still appears to be that the new, replacement drive {\tt
  /dev/sda} in port 0, has been mislabeled as part of unit 1, when it ought to
be part of unit 0 alongside the drive in port 1, {\tt /dev/sdb} to form a RAID-1
array. The RAID-1 array still exists as unit 0, albeit in a DEGRADED state for
the time being. Let's see what we can do about changing {\tt p0}'s unit
affiliation from unit 1 to unit 0. 

\qq We found some interesting instructions! It looks like we can delete the
extra unit, unit 1, then tell the drive to make itself a part of the degraded
array. Let's give it a shot! First, we delete unit 1: {\tt tw\_cli maint
  deleteunit c0 u1}. Then, we explicitly begin the rebuild: {\tt tw\_cli /c0/u0
  start rebuild disk=0}. Unit 0 is being properly rebuilt! The status of the
rebuild process can be monitored with {\tt tw\_cli /c0/u0 show rebuildstatus}. 

\qq The drive was successfully rebuilt! Unfortunately, two of the drives in the
first {\tt raidz2} group decided to fail, {\tt /dev/sdg} and {\tt /dev/sdh}. We
threw the two spares in to replace them with {\tt zpool replace nas0 <number of
  failed drive> sdx}, where {\tt sdx} is the identifier for the spare. While
those were resilvering, yet another drive, {\tt /dev/sdf}, failed in the same
{\tt raidz2} group as the other two failed drives. We've now got three drives
rebuilding in one {\tt raidz2} group, which has a tolerance of two drive
failures. The progress of the scan is still increasing, however, so we remain
hopeful. 

\end{document}
